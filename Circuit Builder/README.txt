The virtual reality environment was developed using Unity’s XR Interaction Toolkit and OpenXR framework. We followed tutorials on hand animation and interaction setup, importing the Animated Hands asset that includes prefabs, skeletal structures, and materials for both hands. XR components were implemented to support object selection, grabbing, release, and haptic feedback for both hands, and were later customized to align with our environment’s needs.
Locomotion systems including movement, turning, and teleportation were configured using OpenXR settings based on tutorial guidance and adjusted for smooth, natural navigation within the scene. Lighting across the environment was implemented entirely by us to meet the specified visual and functional requirements.
Custom scripts were developed to enhance object interactions, manage grabbable elements, control hand animations, enable pinch interactions, and handle teleportation activation and interaction logging. These were written to extend and refine the base capabilities of the XR Interaction Toolkit.
We utilized existing XR and XRI assets provided through Unity’s Package Manager, such as prefabs for interaction, teleportation, and user interface elements, as well as shaders and textures for materials and environment rendering. The Universal Render Pipeline (URP) was configured for optimized performance and visual quality.
Materials, textures, and HDRI skyboxes were applied to create realistic surfaces and lighting. Sound effects and an AudioManager were integrated to support immersive feedback for user interactions. Additionally, recovery scenes and configurations for MockHMD and OpenXR loaders were set up to ensure stable testing and deployment.
All integration, customization, and scripting were implemented by us, while tutorials were referenced only for initial setup guidance.

